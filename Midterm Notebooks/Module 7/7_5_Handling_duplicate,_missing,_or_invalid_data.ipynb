{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Handling duplicate, missing, or invalid data\n"
      ],
      "metadata": {
        "id": "7zxVq3EKoX1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About the data"
      ],
      "metadata": {
        "id": "6VerCRSWocSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will using daily weather data that was taken from the National Centers for Environmental Information (NCEI) API and altered to introduce many common\n",
        "problems faced when working with data.\n",
        "\n",
        "Note: The NCEI is part of the National Oceanic and Atmospheric Administration (NOAA) and, as you can see from the URL for the API, this resource was created when the\n",
        "NCEI was called the NCDC. Should the URL for this resource change in the future, you can search for the NCEI weather API to find the updated one"
      ],
      "metadata": {
        "id": "ELxcYLpmpNcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background Of the Data"
      ],
      "metadata": {
        "id": "4llo9pKepRtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data meanings:\n",
        "- PRCP : precipitation in millimeters\n",
        "- SNOW : snowfall in millimeters\n",
        "- SNWD : snow depth in millimeters\n",
        "- TMAX : maximum daily temperature in Celsius\n",
        "- TMIN : minimum daily temperature in Celsius\n",
        "- TOBS : temperature at time of observation in Celsius\n",
        "- WESF : water equivalent of snow in millimeters\n",
        "\n",
        "Some important facts to get our bearings:\n",
        "- According to the National Weather Service, the coldest temperature ever recorded in Central Park was -15°F (-26.1°C) on February 9, 1934: source\n",
        "- The temperature of the Sun's photosphere is approximately 5,505°C: source"
      ],
      "metadata": {
        "id": "1-4EOuB7pWQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "ydY49xo_piG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to import pandas and read in the long-format data to get started"
      ],
      "metadata": {
        "id": "t16KBN-mpkTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/dirty_data.csv\")"
      ],
      "metadata": {
        "id": "zk061gjQob50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding Problematic data"
      ],
      "metadata": {
        "id": "Pkii5tMrqL6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good first step isss to look at some rows"
      ],
      "metadata": {
        "id": "_YUq6w5aqO7V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYiEobnCn0cm"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the summary statistics can reveal strange or missing values"
      ],
      "metadata": {
        "id": "9pkhl7EBqYPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "U46pCxj5qezQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The info() method can pinpoint missing values and wrong data types:"
      ],
      "metadata": {
        "id": "F-PUxwZNqksl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "UaYDwDhQqlf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use pd.isnull() / pd.isna() or the isna() / isnull() method of the series to find nulls"
      ],
      "metadata": {
        "id": "TXQmmMtSquSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contain_nulls = df[\n",
        "df.SNOW.isnull() | df.SNWD.isna()\\\n",
        "| pd.isnull(df.TOBS) | pd.isna(df.WESF)\\\n",
        "| df.inclement_weather.isna()]\n",
        "\n",
        "contain_nulls.shape[0]"
      ],
      "metadata": {
        "id": "KYyASA_cqsJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contain_nulls.head(10)"
      ],
      "metadata": {
        "id": "8ZvcdYmsq4Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we can't check if we have NaN like this:"
      ],
      "metadata": {
        "id": "rVFBA9v1q-xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.inclement_weather == 'NaN'].shape[0]"
      ],
      "metadata": {
        "id": "pG5RKsEXq_Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is because it is actually np.nan . However, notice this also doesn't work"
      ],
      "metadata": {
        "id": "MIdnxglBrFiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "df[df.inclement_weather == np.nan].shape[0]"
      ],
      "metadata": {
        "id": "NtPHwHkBrE_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to use one of the methods discussed earlier for this to work"
      ],
      "metadata": {
        "id": "PkREstzzrL0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.inclement_weather.isna()].shape[0]"
      ],
      "metadata": {
        "id": "mfjYYgs0rMS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can find -inf / inf by comparing to -np.inf / np.inf"
      ],
      "metadata": {
        "id": "hGVoEaSnrPam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.SNWD.isin([-np.inf, np.inf])].shape[0]"
      ],
      "metadata": {
        "id": "ck9yJkl7rQwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rather than do this for each column, we can write a function that will use a dictionary comprehension to check all the columns for us:"
      ],
      "metadata": {
        "id": "dbahSYBKrTyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def get_inf_count(df):\n",
        "    \"\"\"Find the number of inf/-inf values per column in the dataframe\"\"\"\n",
        "    return {\n",
        "    col : df[df[col].isin([np.inf, -np.inf])].shape[0] for col in df.columns\n",
        "    }\n",
        "get_inf_count(df)"
      ],
      "metadata": {
        "id": "u6dSMKNerULI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can decide how to handle the infinite values of snow depth, we should look at the summary statistics for snowfall which form a big part in determining the snow\n",
        "depth:"
      ],
      "metadata": {
        "id": "rHnWghXvrbGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame({\n",
        "    'np.inf Snow Depth': df[df.SNWD == np.inf].SNOW.describe(),\n",
        "    '-np.inf Snow Depth': df[df.SNWD == -np.inf].SNOW.describe()\n",
        "}).T"
      ],
      "metadata": {
        "id": "W8r2TaRFrbnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now look into the date and station columns. We saw the ? for station earlier, so we know that was the other unique value. However, we see that some dates are\n",
        "present 8 times in the data and we only have 324 days meaning we are also missing days:"
      ],
      "metadata": {
        "id": "EfZCPpF6rex6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include='object')"
      ],
      "metadata": {
        "id": "5SYFPdXyreW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the duplicated() method to find duplicate rows:"
      ],
      "metadata": {
        "id": "_LYRnmMnruGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.duplicated()].shape[0]"
      ],
      "metadata": {
        "id": "4tpBQb-4rvaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The default for keep is 'first' meaning it won't show the first row that the duplicated data was seen in; we can pass in False to see it though"
      ],
      "metadata": {
        "id": "eXEDPqnnry1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.duplicated(keep=False)].shape[0]"
      ],
      "metadata": {
        "id": "2zt-9-_2ryYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also specify the columns to use:"
      ],
      "metadata": {
        "id": "9x3ifrfsr3mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.duplicated(['date', 'station'])].shape[0]"
      ],
      "metadata": {
        "id": "M0GUcfXOr4E6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at a few duplicates. Just in the few values we see here, we know that the top 4 are actually in the data 6 times because by default we aren't seeing their first\n",
        "occurrence:"
      ],
      "metadata": {
        "id": "MZA2jPrnr6Ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.duplicated()].head()"
      ],
      "metadata": {
        "id": "R-wURxMUr7xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mitigating Issues"
      ],
      "metadata": {
        "id": "UVQaYZTdr-t4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling duplicated data"
      ],
      "metadata": {
        "id": "jsHMLuN1sA-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we know we have NY weather data and noticed we only had two entries for station , we may decide to drop the station column because we are only interested in\n",
        "the weather data. However, when dealing with duplicate data, we need to think of the ramifications of removing it. Notice we only have data for the WESF column when the\n",
        "station is ? :"
      ],
      "metadata": {
        "id": "SJPUEjqCsGGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.WESF.notna()].station.unique()"
      ],
      "metadata": {
        "id": "smIWC-XEsAtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we determine it won't impact our analysis, we can use drop_duplicates() to remove them"
      ],
      "metadata": {
        "id": "ge5wUBcDsMK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save this information for later\n",
        "station_qm_wesf = df[df.station == '?'].WESF\n",
        "\n",
        "# sort ? to the bottom\n",
        "df.sort_values('station', ascending=False, inplace=True)\n",
        "\n",
        "# drop duplicates based on the date column keeping the first occurrence\n",
        "# which will be the valid station if it has data\n",
        "df_deduped = df.drop_duplicates('date').drop(\n",
        "    # remove the station column because we are done with it\n",
        "    # and WESF because we need to replace it later\n",
        "    columns=['station', 'WESF']\n",
        ").sort_values('date').assign( # sort by the date\n",
        "# add back the WESF column which will be properly matched because of the index\n",
        "WESF=station_qm_wesf\n",
        ")\n",
        "df_deduped.shape"
      ],
      "metadata": {
        "id": "bjrcKU6VsL_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out the 4th row, we have WESF in the correct spot thanks to the index:"
      ],
      "metadata": {
        "id": "gZxINnYosfp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped.head()"
      ],
      "metadata": {
        "id": "RsnOXqTcsf_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dealing with nulls"
      ],
      "metadata": {
        "id": "pGiWdmMwskJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could drop nulls, replace them with some arbitrary value, or impute them using the surrounding data. Each of these options may have ramifications, so we must choose\n",
        "wisely.\n",
        "\n",
        "We can use dropna() to drop rows where any column has a null value. The default options leave us without data:"
      ],
      "metadata": {
        "id": "ax6T6rX7soKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped.dropna().shape"
      ],
      "metadata": {
        "id": "34k_KIUyspA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we pass how='all' , we can choose to only drop rows where everything is null, but this removes nothing:"
      ],
      "metadata": {
        "id": "ocx91xqIstDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped.dropna(how='all').shape"
      ],
      "metadata": {
        "id": "Zr5zwNAkstXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use just a subset of columns to determine what to drop with the subset argumen"
      ],
      "metadata": {
        "id": "kO3R_j7Es0vZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped.dropna(\n",
        "  how='all', subset=['inclement_weather', 'SNOW', 'SNWD']\n",
        ").shape"
      ],
      "metadata": {
        "id": "KLiTm9HCsxcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can also be performed along columns, and we can also require a certain number of null values before we drop the data:"
      ],
      "metadata": {
        "id": "UvuPALX4s6Cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped.dropna(\n",
        "    axis='columns',\n",
        "    thresh=df_deduped.shape[0]*.75).columns"
      ],
      "metadata": {
        "id": "B1Qz0F7Ps7hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can choose to fill in the null values instead with fillna() :"
      ],
      "metadata": {
        "id": "ZsvaWCz3tA-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped.loc[:,'WESF'].fillna(0, inplace=True)\n",
        "df_deduped.head()"
      ],
      "metadata": {
        "id": "YDCR3o0FtDYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point we have done every we can without distorting the data. We know that we are missing dates, but if we reindex, we don't know how to fill in the NaN data. With the\n",
        "weather data, we can't assume because it snowed one day that it will snow the next or that the temperature will be the same. For this reason, note that the next few examples\n",
        "are just for illustrative purposes only—just because we can do something doesn't mean we should\n",
        "\n",
        "That being said, let's try to address some of remaining issues with the temperature data. We know that when TMAX is the temperature of the Sun, it must be because there\n",
        "was no measured value, so let's replace it with NaN and then we will make an assumption that the temperature won't change drastically day-to-day. Note that this is actually\n",
        "a big assumption, but it will allow us to understand how fillna() works when we provide a strategy through the method parameter. We will also do this for TMIN which\n",
        "currently uses -40°C for its placeholder when we know that the coldest temperature ever recorded in NYC was -15°F (-26.1°C) on February 9, 1934."
      ],
      "metadata": {
        "id": "p85HNtCntFrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fillna() method gives us 2 options for the method parameter:\n",
        "- 'ffill' to forward fill\n",
        "- 'bfill' to back fill\n",
        "\n",
        "Note that 'nearest' is missing because we are not reindexing.\n",
        "\n",
        "Here, we will use 'ffill' to show how this works:"
      ],
      "metadata": {
        "id": "epCShZ5mtJms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped.assign(\n",
        "  TMAX=lambda x: x.TMAX.replace(5505, np.nan).fillna(method='ffill'),\n",
        "  TMIN=lambda x: x.TMIN.replace(-40, np.nan).fillna(method='ffill')\n",
        ").head()"
      ],
      "metadata": {
        "id": "d3LRBmBwtQPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use np.nan_to_num() to turn np.nan into 0 and -np.inf / np.inf into large negative or positive finite numbers:"
      ],
      "metadata": {
        "id": "0RfvmLTYtUOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped.assign(\n",
        "  SNWD=lambda x: np.nan_to_num(x.SNWD)\n",
        ").head()"
      ],
      "metadata": {
        "id": "k_X_UcGLtYcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can couple fillna() with other types of calculations for interpolation. Here we replace missing values of TMAX with the median of all TMAX values, TMIN with the\n",
        "median of all TMIN values, and TOBS to the average of the TMAX and TMIN values. Since we place TOBS last, we have access to the imputed values for TMIN and\n",
        "TMAX in the calculation. WARNING: the text has a typo and fills in TMAX with TMIN's median, the below is correct.:"
      ],
      "metadata": {
        "id": "9LVLI4KbtbNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped.assign(\n",
        "  TMAX=lambda x: x.TMAX.replace(5505, np.nan).fillna(x.TMAX.median()),\n",
        "  TMIN=lambda x: x.TMIN.replace(-40, np.nan).fillna(x.TMIN.median()),\n",
        "  # average of TMAX and TMIN\n",
        "  TOBS=lambda x: x.TOBS.fillna((x.TMAX + x.TMIN) / 2)\n",
        ").head()"
      ],
      "metadata": {
        "id": "8N9qaghLtdLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use apply() for running the same calculation across columns. For example, let's fill all missing values with their rolling 7 day median of their values, setting the\n",
        "number of periods required for the calculation to 0 to ensure we don't introduce more extra NaN values. (Rolling calculations will be covered in chapter 4.) We need to set\n",
        "the date column as the index so apply() doesn't try to take the rolling 7 day median of the date"
      ],
      "metadata": {
        "id": "RjrCCSu9tgTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped.assign(\n",
        "  # make TMAX and TMIN NaN where appropriate\n",
        "  TMAX=lambda x: x.TMAX.replace(5505, np.nan),\n",
        "  TMIN=lambda x: x.TMIN.replace(-40, np.nan)\n",
        ").set_index('date').apply(\n",
        "    # rolling calculations will be covered in chapter 4, this is a rolling 7 day median\n",
        "    # we set min_periods (# of periods required for calculation) to 0 so we always get a result\n",
        "    lambda x: x.fillna(x.rolling(7, min_periods=0).median())\n",
        ").head(10)"
      ],
      "metadata": {
        "id": "T5I_VBqotgvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last strategy we could try is interpolation with the interpolate() method. We specify the method parameter with the interpolation strategy to use. There are many\n",
        "options, but we will stick with the default of 'linear' , which will treat values as evenly spaced and place missing values in the middle of existing ones. We have some\n",
        "missing data, so we will reindex first. Look at January 9th, which we didn't have before—the values for TMAX , TMIN , and TOBS are the average of values the day prior\n",
        "(January 8th) and the day after (January 10th):"
      ],
      "metadata": {
        "id": "M8Auh4jjtpFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped.assign(\n",
        "  # make TMAX and TMIN NaN where appropriate\n",
        "  TMAX=lambda x: x.TMAX.replace(5505, np.nan),\n",
        "  TMIN=lambda x: x.TMIN.replace(-40, np.nan),\n",
        "  date=lambda x: pd.to_datetime(x.date)\n",
        "  ).set_index('date').reindex(\n",
        "  pd.date_range('2018-01-01', '2018-12-31', freq='D')\n",
        ").apply(\n",
        "  lambda x: x.interpolate()\n",
        ").head(10)"
      ],
      "metadata": {
        "id": "D_I-QDlItq6k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}